{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Get Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Data\n",
    "data = Path(\"Data/\")\n",
    "image_path = data / \"XRAY_DATA\"\n",
    "\n",
    "# Train/Test Directory\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image paths\n",
    "train_image_paths = (list(train_dir.glob(\"*/*.png\")) + \n",
    "                     list(train_dir.glob(\"*/*.jpeg\")) + \n",
    "                     list(train_dir.glob(\"*.jpg\")))\n",
    "\n",
    "test_image_paths = (list(test_dir.glob(\"*/*.png\")) +\n",
    "                    list(test_dir.glob(\"*/*.jpeg\")) +\n",
    "                    list(test_dir.glob(\"*/*.jpg\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Get Data Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image Meta Data For Labels\n",
    "df = pd.read_csv(data / \"Metadata.csv\")\n",
    "\n",
    "# Remove unneeded columns\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.drop('Label_2_Virus_category', axis=1, inplace=True)\n",
    "df.drop('Label_1_Virus_category', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Sort Data into Classes based on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort image into folders by label and delete files not in the dataframe\n",
    "for image_path in train_image_paths:\n",
    "    image_name = image_path.name\n",
    "    if image_name not in df['X_ray_image_name'].values:\n",
    "        os.remove(image_path)\n",
    "    else:\n",
    "        label = df[df['X_ray_image_name'] == image_name]['Label'].values[0]\n",
    "        if label == 'Normal':\n",
    "            os.rename(image_path, train_dir / \"Normal\" / image_name)\n",
    "        elif label == 'Pnemonia':\n",
    "            os.rename(image_path, train_dir / \"Pnemonia\" / image_name)\n",
    "\n",
    "for image_path in test_image_paths:\n",
    "    image_name = image_path.name\n",
    "    if image_name not in df['X_ray_image_name'].values:\n",
    "        os.remove(image_path)\n",
    "    else:\n",
    "        label = df[df['X_ray_image_name'] == image_name]['Label'].values[0]\n",
    "        if label == 'Normal':\n",
    "            os.rename(image_path, test_dir / \"Normal\" / image_name)\n",
    "        elif label == 'Pnemonia':\n",
    "            os.rename(image_path, test_dir / \"Pnemonia\" / image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and Load Data with Torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all of the images the same size\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize([512, 512]),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_data = datasets.ImageFolder(root = train_dir,\n",
    "                                  transform=img_transforms,\n",
    "                                  target_transform=None)\n",
    "\n",
    "test_data = datasets.ImageFolder(root= test_dir,\n",
    "                                 transform=img_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into dataloaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle = True,\n",
    "                              num_workers= NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False,\n",
    "                             num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim, output_shape):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_dim,\n",
    "                      kernel_size= 7,\n",
    "                      stride = 2,\n",
    "                      padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim,\n",
    "                      out_channels=hidden_dim,\n",
    "                      kernel_size= 7,\n",
    "                      stride = 2,\n",
    "                      padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride = 2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_dim,\n",
    "                      out_channels=hidden_dim,\n",
    "                      kernel_size= 3,\n",
    "                      stride = 1,\n",
    "                      padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim,\n",
    "                      out_channels=hidden_dim,\n",
    "                      kernel_size= 3,\n",
    "                      stride = 1,\n",
    "                      padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride = 2)\n",
    "        )\n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_dim,\n",
    "                      out_channels=hidden_dim,\n",
    "                      kernel_size= 3,\n",
    "                      stride = 1,\n",
    "                      padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim,\n",
    "                      out_channels=hidden_dim,\n",
    "                      kernel_size= 3,\n",
    "                      stride = 1,\n",
    "                      padding = 0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride = 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_dim*12*12,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_3(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = CNN_model(input_shape=3,\n",
    "                  hidden_dim=10,\n",
    "                  output_shape=len(train_data.classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_step()\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer\n",
    "               ):\n",
    "    \n",
    "    # Put the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy value\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward Pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss Backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred) \n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test_step()\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module\n",
    "              ):\n",
    "    \n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Setup test loss and test accuracy value\n",
    "        test_loss, test_acc = 0, 0\n",
    "\n",
    "        # Loop through data loader data batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Move data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward Pass\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # 2. Calculate the loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy metric\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            test_acc += (y_pred_class==y).sum().item()/len(y_pred)\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        test_loss /= len(dataloader)\n",
    "        test_acc /= len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 0.5707263377416565 | Train Acc: 0.7435366465863453 | Test Loss: 0.6399790181054009 | Test Acc: 0.6822916666666666\n",
      "Epoch: 1 | Train Loss: 0.5680739904742643 | Train Acc: 0.7465486947791165 | Test Loss: 0.6296020613776313 | Test Acc: 0.6822916666666666\n",
      "Epoch: 2 | Train Loss: 0.5666707755930452 | Train Acc: 0.7473644578313253 | Test Loss: 0.629071788655387 | Test Acc: 0.6822916666666666\n",
      "Epoch: 3 | Train Loss: 0.5675142143505165 | Train Acc: 0.7465486947791165 | Test Loss: 0.6594715093572935 | Test Acc: 0.6822916666666666\n",
      "Epoch: 4 | Train Loss: 0.5681091907871775 | Train Acc: 0.7465486947791165 | Test Loss: 0.6320347471369637 | Test Acc: 0.6822916666666666\n",
      "Epoch: 5 | Train Loss: 0.5634685397507196 | Train Acc: 0.7465486947791165 | Test Loss: 0.8868400640785694 | Test Acc: 0.6822916666666666\n",
      "Epoch: 6 | Train Loss: 0.24273062284183072 | Train Acc: 0.9056852409638554 | Test Loss: 0.41233713469571537 | Test Acc: 0.819264846743295\n",
      "Epoch: 7 | Train Loss: 0.13187221346804537 | Train Acc: 0.9497364457831325 | Test Loss: 0.922063169004913 | Test Acc: 0.7326388888888888\n",
      "Epoch: 8 | Train Loss: 0.1163648270422604 | Train Acc: 0.9576430722891566 | Test Loss: 0.5333333515639728 | Test Acc: 0.8038194444444444\n",
      "Epoch: 9 | Train Loss: 0.10133376844361963 | Train Acc: 0.9631024096385542 | Test Loss: 0.4477057937377443 | Test Acc: 0.8229166666666666\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "# Train and Test Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_step(model = model,\n",
    "                                       dataloader = train_dataloader,\n",
    "                                       loss_fn= loss_fn,\n",
    "                                       optimizer= optimizer)\n",
    "    \n",
    "    test_loss, test_acc = test_step(model = model,\n",
    "                                    dataloader = test_dataloader,\n",
    "                                    loss_fn= loss_fn)\n",
    "    \n",
    "    print(f\"Epoch: {epoch} | Train Loss: {train_loss:.2f} | Train Acc: {train_acc:.2f} | Test Loss: {test_loss:.2f} | Test Acc: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features and Use SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            features.append(y_pred.cpu())\n",
    "            labels.append(y.cpu())\n",
    "    return torch.cat(features), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(model, train_dataloader) \n",
    "test_features, test_labels = extract_features(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Train SVM\n",
    "def train_svm(features, labels):\n",
    "    clf = SVC()\n",
    "    clf.fit(features, labels)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.21\n"
     ]
    }
   ],
   "source": [
    "# Test SVM\n",
    "clf = train_svm(train_features, train_labels)\n",
    "y_pred = clf.predict(test_features)\n",
    "\n",
    "error = np.mean(y_pred != test_labels.numpy())\n",
    "print(f\"Error: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
